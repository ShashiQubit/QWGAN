# -*- coding: utf-8 -*-
"""QWGAN_qiio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12JC7kEVQLor3vcYm2zHj9eQMyTKa16Bx

# Generating Financial Time Series with a Quantum Wasserstein GAN

### PQC Characteristics
 - Rotation layers (RX*RY*RZ - Parameterized)
 - Entangling layers (CZ - All-to-all - Not Parameterized)
"""

#!sudo apt install msttcorefonts -qq
#!rm ~/.cache/matplotlib -rf

# hide tensorflow warnings on GPU execution (we will use CPU)

from silence_tensorflow import silence_tensorflow

silence_tensorflow()


# disable GPU usage

import os

os.environ["CUDA_VISIBLE_DEVICES"] = ""

import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm, wasserstein_distance, probplot
from scipy.special import lambertw
from statsmodels.graphics import tsaplots
import statsmodels.api as sm
import time
import cirq, random, sympy
import tensorflow_quantum as tfq
import tensorflow as tf
from cirq.contrib.svg import SVGCircuit
from cirq.circuits import InsertStrategy


import pandas as pd



# limit GPU memory growth to avoid OUT_OF_MEMORY issues (before initializing GPU -- if used)


gpus = tf.config.experimental.list_physical_devices('GPU')


if gpus:
    try:
        tf.config.experimental.set_memory_growth(gpus[0], True)
    except RuntimeError as e:
        print(e)



# Load the data from the saved CSV file with a standard single-level header

qiio = pd.read_csv('iio.csv')


# Convert the 'Close' column to numeric, coercing errors

qiio['value'] = pd.to_numeric(qiio['value'], errors='coerce')


# Drop any rows with missing 'Close' values after coercion

qiio.dropna(subset=['value'], inplace=True)
start_date = '2013-10-11'
qiio = qiio[qiio['timestamp'] >= start_date].copy()

# Convert the numeric 'Close' values to a TensorFlow tensor

qiio_close = tf.convert_to_tensor(qiio['value'].values)



# display the shape of the numpy array

print('qiio shape (total days): ', tf.shape(qiio_close))



# check if GPU is available

print("GPU Available:", tf.config.list_physical_devices('GPU'))


# check which GPU is being used

print("GPU Device:", tf.test.gpu_device_name())


# Use the DataFrame's index for the date information

date = qiio.index


# plot the qiio data

plt.plot(date, qiio_close)

plt.xlabel('Days')

plt.ylabel('qiio Closing Value')

plt.title('qiio Index')

plt.grid()

plt.savefig("plots/raw_series_of_qiio.svg", format="svg")
plt.close()

# direct returns over time
qiio_direct_r = qiio_close[1:] - qiio_close[:-1]

# logarithmic returns over time
qiio_log_r = np.log(qiio_close[1:]) - np.log(qiio_close[:-1])

# plot the graphs side-by-side
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))

axes[0].plot(date[1:], qiio_direct_r)
axes[0].set_title('qiio Index Direct Returns')
axes[0].grid()

axes[1].plot(date[1:], qiio_log_r)
axes[1].set_title('qiio Index Log Returns')
axes[1].grid()

# Show the plot
plt.savefig("plots/qiio Index Direct Returns_and_qiio Index Log Returns.svg", format="svg")
plt.close()

# plot the graphs side-by-side
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))

# density of log-returns
bin_edges = np.linspace(-0.05, 0.05, num=50)  # define the bin edges
bin_width = bin_edges[1] - bin_edges[0]
bin_edges = np.append(bin_edges, bin_edges[-1] + bin_width)
axes[0].hist(qiio_log_r, bins=bin_edges, density=True, width=0.001, label='Log-returns density')
axes[0].grid()

# normal distribution with same mean and standard deviation as log-returns
mu = np.mean(qiio_log_r)
sigma = np.std(qiio_log_r)

# Generate a set of points x
x = np.linspace(-0.05, 0.05, 100)

# Generate the Gaussian PDF for the points x with same mean and standard deviation as the log-returns
pdf = norm.pdf(x, mu, sigma)

# plot the Gaussian PDF
axes[0].plot(x, pdf, 'r', label='Gaussian distribution')
axes[0].legend()

# plot in logarithmic scale
axes[1].hist(qiio_log_r, bins=bin_edges, density=True, width=0.001, log=True)
axes[1].grid()

# plot the Gaussian PDF in logarithmic scale
axes[1].semilogy(x, pdf, 'r')

plt.savefig("plots/Gaussian_PDF_and_log_returned_density.svg", format="svg")
plt.close()

# convert the qiio log returns to a TensorFlow tensor
qiio_log_r_tf = tf.convert_to_tensor(qiio_log_r)
# plot the ACF for the specified lags
tsaplots.plot_acf(qiio_log_r_tf, lags=18, zero=False)
plt.xlabel('Lags')
plt.title('ACF qiio Log-Returns')
plt.ylabel(r'$\rho$')
plt.grid()
plt.savefig("plots/ACF qiio Log-Returns.svg", format="svg")
plt.close()

# plot the ACF of absolute log-returns for the specified lags
tsaplots.plot_acf(tf.abs(qiio_log_r_tf), lags=18, zero=False)
plt.xlabel('Lags')
plt.title('ACF qiio Absolute Log-Returns')
plt.ylabel(r'$\rho_{abs}$')
plt.grid()
plt.savefig("plots/ACF qiio Absolute Log-Returns.svg", format="svg")
plt.close()

lags = range(1, 19)
lev = []
for lag in lags:
    # slice the tensors to get the appropriate lagged sequences
    r_t = qiio_log_r_tf[:-lag]
    squared_lag_r = tf.square(tf.abs(qiio_log_r_tf[lag:]))

    # calculate the leverage effect
    # calculate the correlation coefficient
    correlation_matrix = np.corrcoef(r_t, squared_lag_r)
    lev.append(correlation_matrix[0, 1])

# plot the the levarage effect
plt.plot(lev)
plt.title('Leverage Effect qiio')
plt.xlabel('Lags')
plt.ylabel(r'$L(\tau)$')
plt.grid()
plt.savefig("plots/Leverage Effect qiio.svg", format="svg")
plt.close()

# Generate the Q-Q plot
plt.figure(figsize=(6, 4))
probplot(qiio_log_r_tf, dist='norm', plot=plt)
plt.title('Q-Q Plot - qiio Log Returns')
plt.xlabel('Theoretical Quantiles')
plt.ylabel('Sample Quantiles')
plt.grid(True)
plt.savefig("plots/Q-Q Plot - qiio Log Returns.svg", format="svg")
plt.close()

"""## Implementation

### Data Pre-Processing

#### (1) Normalization
"""

def normalize(data):
    mu = tf.reduce_mean(data)
    std = tf.math.reduce_std(data)

    return (data - mu)/std

def denormalize(norm_data, mu_original, std_original):
    return norm_data*std_original + mu_original

# normalize the log-returns
qiio_norm_r = normalize(qiio_log_r_tf)
# display the mean and standard deviation of the original log-returns
print(f'Original qiio log-returns mean = {tf.reduce_mean(qiio_log_r_tf)}, std = {tf.math.reduce_std(qiio_log_r_tf)}')
# display the mean and standard deviation of the normalized log-returns
print(f'Normalized qiio log-returns mean = {tf.reduce_mean(qiio_norm_r)}, std = {tf.math.reduce_std(qiio_norm_r)}')

print('Original Data Min-Max')
print(tf.reduce_min(qiio_log_r_tf).numpy(), tf.reduce_max(qiio_log_r_tf).numpy())

print('Normalized Data Min-Max')
print(tf.reduce_min(qiio_norm_r).numpy(), tf.reduce_max(qiio_norm_r).numpy())

fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(7,4))

# density of log-returns
bin_edges = np.linspace(-6, 6, num=100)  # define the bin edges
bin_width = bin_edges[1] - bin_edges[0]
bin_edges = np.append(bin_edges, bin_edges[-1] + bin_width)
axes.hist(qiio_norm_r, bins=bin_edges, density=True, width=0.1, label='Normalized density')
axes.grid()

# normal distribution with same mean and standard deviation as log-returns
mu = np.mean(qiio_norm_r)
sigma = np.std(qiio_norm_r)

# Generate a set of points x
x = np.linspace(-6, 6, 100)

# Generate the Gaussian PDF for the points x with same mean and standard deviation as the log-returns
pdf = norm.pdf(x, mu, sigma)

# plot the Gaussian PDF
axes.plot(x, pdf, 'r', label='Gaussian')
axes.legend()

plt.savefig("plots/Gaussian_PDF_and_Normalized_density.svg", format="svg")
plt.close()

"""#### (2) Inverse Lambert W Transform"""

def inverse_lambert_w_transform(data, delta):
    """
    Apply inverse Lambert W transform to the input data using the specified delta value.

    Parameters:
    - data: Input data tensor
    - delta: Delta value for the transform (tail parameter)

    Returns:
    - Transformed data array
    """
    sign = tf.sign(tf.cast(data, dtype=tf.float64))
    transformed_data = sign * tf.cast(tf.sqrt(lambertw(delta * data ** 2).real / delta), dtype=tf.float64)

    return transformed_data

def lambert_w_transform(transformed_data, delta, clip_low=-12.0, clip_high=11.0):
    """
    Transform the Gaussianized data back to its original state.

    Parameters:
    - transformed_data: Input data array which was transformed using inverse Lambert W
    - delta: Delta value for the transform (tail parameter)

    Returns:
    - Original Data
    """
    reversed_data = transformed_data*tf.cast(tf.exp((delta/2) * transformed_data ** 2), dtype=tf.float64)
    return tf.clip_by_value(reversed_data, clip_low, clip_high)

# apply inverse Lambert W transform to the normalized log-returns
transformed_qiio = inverse_lambert_w_transform(qiio_norm_r, 1)

fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(7,4))

# density of normalized log-returns
bin_edges = np.linspace(-3, 3, num=50)  # define the bin edges
bin_width = bin_edges[1] - bin_edges[0]
bin_edges = np.append(bin_edges, bin_edges[-1] + bin_width)
axes.hist(transformed_qiio, bins=bin_edges, density=True, width=0.1, label='Transformed log-returns')
axes.grid()

# normal distribution with same mean and standard deviation as log-returns
mu = np.mean(transformed_qiio)
sigma = np.std(transformed_qiio)

# Generate a set of points x
x = np.linspace(-3, 3, 100)

# Generate the Gaussian PDF for the points x with same mean and standard deviation as the normalized log-returns
pdf = norm.pdf(x, mu, sigma)

# plot the Gaussian PDF
axes.plot(x, pdf, 'r', label='Gaussian')
axes.legend()

plt.savefig("plots/Gaussian_PDF_and_Transformed_log_returns.svg", format="svg")
plt.close()

# Generate the Q-Q plot
plt.figure(figsize=(6, 4))
probplot(transformed_qiio, dist='norm', plot=plt)
plt.title('Q-Q Plot - Transformed qiio Log Returns')
plt.xlabel('Theoretical Quantiles')
plt.ylabel('Sample Quantiles')
plt.grid(True)
plt.savefig("plots/Q-Q Plot - Transformed qiio Log Returns.svg", format="svg")
plt.close()

print('Transformed Data Min-Max')
print(tf.reduce_min(transformed_qiio).numpy(), tf.reduce_max(transformed_qiio).numpy())

print('Transformed data mean: ', tf.reduce_mean(transformed_qiio).numpy())
print('Transformed data std: ', tf.math.reduce_std(transformed_qiio).numpy())

"""#### (3) Scale to [-1,1]

Necessary scaling to match the range of expectation values.
"""

min_val = tf.reduce_min(transformed_qiio)
max_val = tf.reduce_max(transformed_qiio)
scaled_data = -1.0 + 2.0 * (transformed_qiio - min_val) / (max_val - min_val)

print(f'Scaled Normalized Transformed log-returns mean = {tf.reduce_mean(scaled_data)}, std = {tf.math.reduce_std(scaled_data)}')
print('Scaled Normalized Transformed log-returns min-max: ', tf.reduce_min(scaled_data).numpy(), tf.reduce_max(scaled_data).numpy())

# a function to scale back from [-1,1] to the previous range
def rescale(scaled_data, rescaled_data):
    min_val = tf.reduce_min(rescaled_data)
    previous_data = 0.5 * (scaled_data + 1.0) * (max_val - min_val) + min_val

    return previous_data

"""#### (4) Rolling Window"""

def rolling_window(data, m, s):
    return tf.map_fn(lambda i: data[i:i+m], tf.range(0, len(data) - m + 1, s), dtype=tf.float64)

"""## qGAN"""

class qGAN(tf.keras.Model):

    def __init__(self, num_epochs, batch_size, window_length, n_critic, gp, num_layers, num_qubits):
        super(qGAN, self).__init__()

        # classical hyperparameters
        self.num_epochs = num_epochs
        self.batch_size = batch_size
        self.window_length = window_length
        self.n_critic = n_critic
        self.gp = gp

        # quantum hyperparameters
        # one layer corresponds to a rotation and an entangling layer together
        self.num_layers = num_layers
        self.num_qubits = num_qubits

        # quantum circuit settings
        self.qubits = cirq.GridQubit.rect(1, num_qubits)
        # create the set of Pauli strings to measure -> {X1, Z1, X2, Z2, etc}
        # X1 means we measure the first qubit only with X, Z1 the first qubit only with Z and so on...
        self.measurements = []
        for qubit in self.qubits:
            self.measurements.append(cirq.X(qubit))
            self.measurements.append(cirq.Z(qubit))

        # number of parameters of the PQC and re-uploading layers
        self.num_params = self.count_params()

        # define the trainable parameters of the PQC main and re-uploading layers (trainable)
        self.params_pqc = [sympy.Symbol(f'theta{i}') for i in range(self.num_params)]

        # define the classical critic network (CNN)
        self.critic = self.define_critic_model(window_length)
        # define the quantum generator network (PQC)
        self.generator = self.define_generator_model()

        # monitoring purposes
        # average critic and generator losses for each epoch
        self.critic_loss_avg = []
        self.generator_loss_avg = []
        # Earth's mover distance (EMD) for each epoch
        self.emd_avg = []
        # stylized facts RMSEs for each epoch
        self.acf_avg = []
        self.vol_avg = []
        self.lev_avg = []

    ####################################################################################
    #
    # count the parameters of the quantum circuit
    #
    ####################################################################################
    def count_params(self):

        # rotation layer with Rx, Ry, Rz has 3N parameters, where N is the number of qubits
        # the entangling layer is not parameterized
        num_params_pqc = 3*self.num_qubits*self.num_layers

        # also, count the parameters of the re-uploading layer that is sandwiched between rotation-entangling layers
        # there is one re-uploading layer after each rotation-entangling layer with a parameterized Rx gate,
        # so the number of parameters is equal to the number of qubits for each re-uploading layer
        num_params_upload = self.num_layers*self.num_qubits

        # the last layer of the PQC is a rotation layer
        num_params_pqc += 3*self.num_qubits

        return num_params_pqc+num_params_upload

    ####################################################################################
    #
    # the classical critic model as a convolutional network
    #
    ####################################################################################
    def define_critic_model(self, window_length):
        model = tf.keras.Sequential()
        model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=10, strides=1, input_shape=(window_length, 1), padding='same'))
        model.add(tf.keras.layers.LeakyReLU(alpha=0.1))

        model.add(tf.keras.layers.Conv1D(filters=128, kernel_size=10, strides=1, padding='same'))
        model.add(tf.keras.layers.LeakyReLU(alpha=0.1))

        model.add(tf.keras.layers.Conv1D(filters=128, kernel_size=10, strides=1, padding='same'))
        model.add(tf.keras.layers.LeakyReLU(alpha=0.1))

        model.add(tf.keras.layers.Flatten())

        model.add(tf.keras.layers.Dense(32, dtype=tf.float64))
        model.add(tf.keras.layers.LeakyReLU(alpha=0.1))
        model.add(tf.keras.layers.Dropout(0.2))

        model.add(tf.keras.layers.Dense(1, dtype=tf.float64))

        return model

    ####################################################################################
    #
    # the encoding layer: resolve the parameters by uniform noise values,
    # used to prepare the initial state for the generator circuit
    #
    ####################################################################################
    def encoding_layer(self, noise_params):

        return cirq.Circuit(cirq.Rx(rads=noise_params[i])(self.qubits[i]) for i in range(self.num_qubits))

    ####################################################################################
    #
    # the quantum generator as a PQC with All-to-all topology for the entangling layer
    #
    ####################################################################################
    def define_generator_circuit(self):

        # cirq circuit
        pqc = cirq.Circuit()

        # index for the parameter tensor of the PQC main and re-uploading layers
        idx = 0

        for layer in range(self.num_layers):
            ###############################################################
            #
            # single-qubit rotation layer
            #
            ###############################################################
            for qubit in self.qubits:
                pqc.append(cirq.Rx(rads=self.params_pqc[idx])(qubit))
                idx += 1
                pqc.append(cirq.Ry(rads=self.params_pqc[idx])(qubit))
                idx += 1
                pqc.append(cirq.Rz(rads=self.params_pqc[idx])(qubit))
                idx += 1

            ###############################################################
            #
            # entangling layer (not parameterized)
            #
            ###############################################################
            for qubit1 in range(self.num_qubits):
                for qubit2 in range(qubit1+1, self.num_qubits):
                    pqc.append(cirq.CZ(self.qubits[qubit1], self.qubits[qubit2]), strategy=InsertStrategy.NEW)

            ###############################################################
            #
            # re-uploading layer with Rx rotation
            # (set the strategy for better readability and understanding)
            #
            ###############################################################
            for i, qubit in enumerate(self.qubits):
                if i == 0:
                    pqc.append(cirq.Rx(rads=self.params_pqc[idx])(qubit), strategy=InsertStrategy.NEW)
                else:
                    pqc.append(cirq.Rx(rads=self.params_pqc[idx])(qubit), strategy=InsertStrategy.INLINE)
                idx += 1

        #####################################################################
        #
        # single-qubit rotation layer as the last layer before measurement
        #
        #####################################################################
        for qubit in self.qubits:
                pqc.append(cirq.Rx(rads=self.params_pqc[idx])(qubit))
                idx += 1
                pqc.append(cirq.Ry(rads=self.params_pqc[idx])(qubit))
                idx += 1
                pqc.append(cirq.Rz(rads=self.params_pqc[idx])(qubit))
                idx += 1

        return pqc

    ####################################################################################
    #
    # the quantum generator model
    #
    ####################################################################################
    def define_generator_model(self):
        # model input
        q_data_input = tf.keras.Input(shape=(), dtype=tf.dtypes.string)
        # define the tensorflow quantum layer (trainable)
        generator = tfq.layers.PQC(self.define_generator_circuit(), self.measurements, repetitions=1000)
        generator_output = generator(q_data_input)
        # tensorflow model
        model = tf.keras.Model(inputs=q_data_input, outputs=generator_output)

        return model

    #############################################################################
    #
    # compile model with given optimizers for critic and generator networks
    #
    #############################################################################
    def compile_QGAN(self, c_optimizer, g_optimizer):
        super(qGAN, self).compile()
        self.c_optimizer = c_optimizer
        self.g_optimizer = g_optimizer

    def train_qgan(self, gan_data, original_data, preprocessed_data, num_elements):
        """
        Parameters:
         - gan_data is the preprocessed dataset with windows for qGAN training
         - original_data is the original qiio log-returns for evaluation of RMSEs (monitoring purposes)
         - preprocessed_data is the preprocessed log-returns without the last normalization step and without windows
          (for reversing the process of generated samples using the mean and std and evaluating the RMSEs)
        """
        for epoch in range(self.num_epochs):
            print(f'Processing epoch {epoch+1}/{self.num_epochs}')
            ################################################################
            #
            # Train the critic for n_critic iterations
            # Process 'batch_size' samples in each iteration
            #
            ################################################################
            # critic loss for 'n_critic' iterations
            critic_t_sum = 0
            for t in range(self.n_critic):
                # record the gradients
                with tf.GradientTape() as critic_tape:
                    # critic loss for 'batch_size' samples
                    critic_sum = 0
                    for i in range(self.batch_size):
                        ###########################################
                        #
                        # Sample real data and a latent variable
                        #
                        ###########################################
                        # shuffle the dataset
                        shuffled_data = gan_data.shuffle(buffer_size=num_elements)
                        # take a single random element from the shuffled dataset
                        random_element = shuffled_data.take(1)
                        # iterate over the random_element dataset to access the value
                        for element in random_element:
                            # access the value of the random element as a tensor
                            real_sample = element
                        # reshape the real sample for compatibility with the first layer of the critic
                        real_sample = tf.reshape(real_sample, (1, self.window_length))

                        ##################################################
                        #
                        # Get the state prepared by the encoding circuit
                        #
                        ##################################################
                        # generate noise parameters for the encoding layer
                        noise_values = np.random.uniform(0, 2 * np.pi, size=self.num_qubits)
                        generator_input_state = self.encoding_layer(noise_values)
                        # convert to tensorflow quantum tensor
                        generator_input = tfq.convert_to_tensor([generator_input_state])
                        # get the fake sample as the expectations of the quantum circuit
                        generated_sample = self.generator(generator_input)
                        generated_sample = tf.cast(generated_sample, dtype=tf.float64)

                        # calculate the critic scores for real and fake samples
                        real_score = self.critic(real_sample)
                        fake_score = self.critic(generated_sample)

                        # compute the gradient penalty
                        gradient_penalty = self.compute_gradient_penalty(real_sample, generated_sample)

                        # calculate the Wasserstein distance loss with gradient penalty
                        critic_loss = fake_score - real_score + self.gp * gradient_penalty
                        # accumulate the critic loss for the sample
                        critic_sum += critic_loss

                    # compute the gradients of critic and apply them
                    critic_gradients = critic_tape.gradient(critic_sum/self.batch_size, self.critic.trainable_variables)
                    self.c_optimizer.apply_gradients(zip(critic_gradients, self.critic.trainable_variables))

                    # accumulate the average critic loss for all samples in this 't' iteration
                    critic_t_sum += critic_sum/self.batch_size

            # average critic loss for this epoch of WGAN training
            self.critic_loss_avg.append(critic_t_sum/self.n_critic)

            ################################################################
            #
            # Train generator for one iteration
            #
            ################################################################
            # sample a batch of input states using the encoding layer
            input_circuits_batch = []
            for _ in range(self.batch_size):
                noise_values = np.random.uniform(0, 2 * np.pi, size=self.num_qubits)
                input_circuits_batch.append(self.encoding_layer(noise_values))

            # convert to tensorflow quantum tensor
            generator_inputs = tfq.convert_to_tensor(input_circuits_batch)

            with tf.GradientTape() as gen_tape:
                # generate fake samples using the generator
                generated_samples = self.generator(generator_inputs)
                generated_samples = tf.cast(generated_samples, dtype=tf.float64)
                # calculate the critic scores for fake samples
                fake_scores = self.critic(generated_samples)
                # calculate the generator loss
                generator_loss = -tf.reduce_mean(fake_scores)

            # compute the gradients of generator and apply them
            generator_gradients = gen_tape.gradient(generator_loss, self.generator.trainable_variables)
            self.g_optimizer.apply_gradients(zip(generator_gradients, self.generator.trainable_variables))

            # average generator loss for this epoch
            self.generator_loss_avg.append(generator_loss)

            ########################################################################################################
            #
            # Calculate the stylized facts RMSEs and the EMD for real and fake data
            #
            # Fake data has shape (num_samples x window_length), with num_samples = original_length / window_length
            # in order to get a time series close to the length of the original
            #
            ########################################################################################################
            # generate noise
            num_samples = len(original_data) // self.window_length
            input_circuits_batch = []
            for _ in range(num_samples):
                noise_values = np.random.uniform(0, 2 * np.pi, size=self.num_qubits)
                input_circuits_batch.append(self.encoding_layer(noise_values))

            # convert to tensorflow quantum tensor
            generator_inputs = tfq.convert_to_tensor(input_circuits_batch)
            # generate fake samples using the generator
            batch_generated = self.generator(generator_inputs)
            # concatenate all time series data into one
            generated_data = tf.reshape(batch_generated, shape=(num_samples*self.window_length,))
            generated_data = tf.cast(generated_data, dtype=tf.float64)
            # rescale
            generated_data = rescale(generated_data, preprocessed_data)
            # reverse the preprocessing on generated sample
            original_norm = lambert_w_transform(generated_data, 1)
            fake_original = denormalize(original_norm, tf.reduce_mean(original_data), tf.math.reduce_std(original_data))
            # calculate the temporal metrics for monitoring the training process
            corr_rmse, volatility_rmse, lev_rmse, emd = self.stylized_facts(original_data, fake_original)
            # store the EMD and RMSEs of stylized facts
            self.acf_avg.append(corr_rmse)
            self.vol_avg.append(volatility_rmse)
            self.lev_avg.append(lev_rmse)
            self.emd_avg.append(emd)

            # checkpoint saving
            if (epoch + 1) % 20 == 0:
                self.generator.save_weights(f"checkpoints/generator_epoch_{epoch+1}.weights.h5")
                self.critic.save_weights(f"checkpoints/critic_epoch_{epoch+1}.weights.h5")

            # print progress every 100 epochs
            if epoch % 100 == 0 or epoch+1 == 3000:
                print(f'\nEpoch {epoch+1} completed')
                print(f'Critic loss (average): {self.critic_loss_avg[epoch][-1][0]}')
                print(f'Generator loss (average): {self.generator_loss_avg[epoch]}')
                print(f'\nEMD (average): {self.emd_avg[epoch]}')
                print(f'ACF RMSE (average): {self.acf_avg[epoch]}')
                print(f'VOLATILITY RMSE (average): {self.vol_avg[epoch]}')
                print(f'LEVERAGE RMSE (average): {self.lev_avg[epoch]}\n')
                print('Min-Max values of original log-returns: ', tf.reduce_min(original_data).numpy(), tf.reduce_max(original_data).numpy())
                print('Min-Max values of generated log-returns (for all batches): ', tf.reduce_min(fake_original).numpy(), tf.reduce_max(fake_original).numpy())
                print('Min-Max values after Lambert: ', tf.reduce_min(original_norm).numpy(), tf.reduce_max(original_norm).numpy())
                print()

    ###########################################################
    #
    # Sample a random number epsilon ~ U[0,1]
    # Create a convex combination of real and generated sample
    # Compute the gradient penalty for the critic network
    #
    ###########################################################
    def compute_gradient_penalty(self, real_sample, generated_sample):
        epsilon = tf.random.uniform((), dtype=tf.float64)
        interpolated_sample = epsilon * real_sample + (1 - epsilon) * generated_sample

        with tf.GradientTape() as tape:
            tape.watch(interpolated_sample)
            scores = self.critic(interpolated_sample)

        gradients = tape.gradient(scores, interpolated_sample)
        gradients_norm = tf.norm(gradients)
        gradient_penalty = (gradients_norm - 1)**2

        return gradient_penalty

    def stylized_facts(self, original_data, fake_original):
        """
        - Calculate the RMSEs of the stylized facts between the original qiio log-returns and
          generated time series

        - Evaluate the EMD between real and generated samples
        """

        ################################################
        #
        # stylized facts for fake samples
        #
        ################################################
        # compute acf for maximum lags = 18
        acf_values = sm.tsa.acf(fake_original, nlags=18)
        # exclude zero lag
        acf_values_generated = tf.convert_to_tensor(acf_values[1:])

        # compute absolute acf (volatility clustering) for maximum lags = 18
        acf_abs_values = sm.tsa.acf(tf.abs(fake_original), nlags=18)
        # exclude zero lag
        acf_abs_values_generated = tf.convert_to_tensor(acf_abs_values[1:])

        # compute leverage effect for maximum lags = 18
        lev = []
        for lag in range(1, 19):
            # slice the tensors to get the appropriate lagged sequences
            r_t = fake_original[:-lag]
            squared_lag_r = tf.square(tf.abs(fake_original[lag:]))

            # calculate the leverage effect
            # calculate the correlation coefficient
            correlation_matrix = np.corrcoef(r_t, squared_lag_r)
            lev.append(correlation_matrix[0, 1])

        leverage_generated = tf.convert_to_tensor(lev)

        ################################################
        #
        # stylized facts for real samples
        #
        ################################################

        # compute acf for maximum lags = 18
        acf_values = sm.tsa.acf(original_data, nlags=18)
        # exclude zero lag
        acf_values_original = tf.convert_to_tensor(acf_values[1:])

        # compute absolute acf (volatility clustering) for maximum lags = 18
        acf_abs_values = sm.tsa.acf(tf.abs(original_data), nlags=18)
        # exclude zero lag
        acf_abs_values_original = tf.convert_to_tensor(acf_abs_values[1:])

        # compute leverage effect for maximum lags = 18
        lev = []
        for lag in range(1, 19):
            # slice the tensors to get the appropriate lagged sequences
            r_t = original_data[:-lag]
            squared_lag_r = tf.square(tf.abs(original_data[lag:]))

            # calculate the leverage effect
            # calculate the correlation coefficient
            correlation_matrix = np.corrcoef(r_t, squared_lag_r)
            lev.append(correlation_matrix[0, 1])

        leverage_original = tf.convert_to_tensor(lev)

        # calculate average RMSEs of stylized facts
        # autocorrelations
        rmse_acf = tf.sqrt(tf.reduce_mean((acf_values_original-acf_values_generated)**2))
        # volatility clustering
        rmse_vol = tf.sqrt(tf.reduce_mean((acf_abs_values_original-acf_abs_values_generated)**2))
        # leverage effect
        rmse_lev = tf.sqrt(tf.reduce_mean((leverage_original-leverage_generated)**2))

        ####################################################################################
        #
        # compute the Earth's mover distance (EMD)
        #
        ####################################################################################
        bin_edges = np.linspace(-1, 1, num=100)  # define the bin edges
        bin_width = bin_edges[1] - bin_edges[0]
        bin_edges = np.append(bin_edges, bin_edges[-1] + bin_width)
        # compute the empirical distribution of original data
        empirical_real, _ = np.histogram(original_data, bins=bin_edges, density=True)
        empirical_real /= np.sum(empirical_real)
        # compute the empirical distribution of generated data
        empirical_fake, _ = np.histogram(fake_original, bins=bin_edges, density=True)
        empirical_fake /= np.sum(empirical_fake)

        # evaluate the EMD using SciPy
        emd = wasserstein_distance(empirical_real, empirical_fake)

        return rmse_acf, rmse_vol, rmse_lev, emd

##################################################################
#
# Hyperparameters
#
##################################################################
WINDOW_LENGTH = 20  # this must be equal to the number of Pauli strings to measure
NUM_QUBITS = 10  # number of qubits
NUM_LAYERS = 4  # number of layers for the PQC

# training hyperparameters
EPOCHS = 1000
BATCH_SIZE = 10

n_critic = 2 # number of iterations for the critic per epoch
LAMBDA = 10  # gradient penalty strength

# instantiate the QGAN model object
qgan = qGAN(EPOCHS, BATCH_SIZE, WINDOW_LENGTH, n_critic, LAMBDA, NUM_LAYERS, NUM_QUBITS)

# set the optimizers
c_optimizer = tf.keras.optimizers.Adam()
g_optimizer = tf.keras.optimizers.Adam()
qgan.compile_QGAN(c_optimizer, g_optimizer)

##################################################################################
#
# Data pre-processing
#
##################################################################################
# apply rolling window in transformed (scaled) log-returns with stride s=5
gan_data_tf = rolling_window(scaled_data, WINDOW_LENGTH, 5)
# create TensorFlow datasets
gan_data = tf.data.Dataset.from_tensor_slices(gan_data_tf)
# get the number of elements in the dataset
num_elements = gan_data.cardinality().numpy()

# display the generator circuit
#SVGCircuit(qgan.define_generator_circuit())

# train the QGAN
print('Training started...')
print('Number of samples to process per epoch: ', num_elements)
print()
start_time_train = time.time()
qgan.train_qgan(gan_data, qiio_log_r_tf, transformed_qiio, num_elements)
exec_time_train = time.time() - start_time_train
print(f'\nQGAN training completed. Training time: --- {exec_time_train/3600:.02f} hours ---')

"""##Save the model"""

# Save weights
qgan.critic.save_weights('critic_weights.h5')
qgan.generator.save_weights('generator_weights.h5')

import numpy as np
np.save('pqc_params.npy', qgan.generator.trainable_variables[0].numpy())

# Load later with:
qgan.critic.load_weights('critic_weights.h5')
qgan.generator.load_weights('generator_weights.h5')

trained_params = dict(zip(qgan.params_pqc, qgan.generator.trainable_variables[0].numpy()))
optimized_circuit = cirq.resolve_parameters(qgan.define_generator_circuit(), trained_params)
print(optimized_circuit)
# code to SAVE qgan arrays in a file like emd_avg , acf etc

# Create a dictionary to store the arrays
qgan_metrics = {
    'emd_avg': qgan.emd_avg,
    'acf_avg': qgan.acf_avg,
    'vol_avg': qgan.vol_avg,
    'lev_avg': qgan.lev_avg,
    'generator_loss_avg': qgan.generator_loss_avg,
    'critic_loss_avg': qgan.critic_loss_avg
}

# Save the dictionary to a file
np.save('qgan_metrics.npy', qgan_metrics)

print("QGAN metrics saved to qgan_metrics.npy")
"""## Plot Training History"""

#critic_loss = tf.squeeze(qgan.critic_loss_avg, axis=(1,2)).numpy()
#generator_loss = np.array(qgan.generator_loss_avg)

critic_loss = np.array(qgan.critic_loss_avg, dtype=np.float64).reshape(-1)
generator_loss = np.array(qgan.generator_loss_avg, dtype=np.float64).reshape(-1)


window = 50
generator_ma = np.convolve(generator_loss, np.ones(window)/window, mode='valid')
critic_ma = np.convolve(critic_loss, np.ones(window)/window, mode='valid')

# plot the graphs side-by-side
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))

# plot the critic loss moving average as a line
axes[0].plot(range(window-1, len(critic_loss)), critic_ma, label='Average Critic Loss', color='blue')
# plot the critic loss
axes[0].plot(critic_loss, color='black', alpha=0.2)

# plot the generator loss moving average as a line
axes[0].plot(range(window-1, len(generator_loss)), generator_ma, label='Average Generator Loss', color='orange')
# plot the generator loss
axes[0].plot(generator_loss, color='black', alpha=0.2)


axes[0].set_ylabel('Loss')
axes[0].legend()
axes[0].grid()

emd_avg = np.array(qgan.emd_avg)
emd_ma = np.convolve(emd_avg, np.ones(window)/window, mode='valid')

axes[1].plot(range(window-1, len(emd_avg)), emd_ma, label='EMD', color='red')
axes[1].plot(emd_avg, color='red', linewidth=0.5, alpha=0.5)

axes[1].set_ylabel('EMD')
axes[1].legend()
axes[1].grid()

acf_avg = np.array(qgan.acf_avg)
vol_avg = np.array(qgan.vol_avg)
lev_avg = np.array(qgan.lev_avg)

acf_ma = np.convolve(acf_avg, np.ones(window)/window, mode='valid')
vol_ma = np.convolve(vol_avg, np.ones(window)/window, mode='valid')
lev_ma = np.convolve(lev_avg, np.ones(window)/window, mode='valid')

# Creating a twin axes for the second graph
axes2 = axes[1].twinx()

axes2.plot(range(window-1, len(acf_avg)), acf_ma, label='ACF', color='green')
axes2.plot(acf_avg, color='green', linewidth=0.5, alpha=0.4)

axes2.plot(range(window-1, len(vol_avg)), vol_ma, label='Volatility Clustering', color='black')
axes2.plot(vol_avg, color='black', linewidth=0.5, alpha=0.3)

axes2.plot(range(window-1, len(lev_avg)), lev_ma, label='Leverage Effect', color='orange')

axes2.set_ylabel('Temporal Metrics')
axes2.legend()
axes2.grid()

# Adjusting the spacing between subplots
plt.tight_layout()
plt.savefig("plots/training_history.svg", format="svg")
plt.close()

"""## Plot Generated Data Properties"""

# generate noise
num_samples = len(qiio_log_r_tf) // WINDOW_LENGTH
input_circuits_batch = []
for _ in range(num_samples):
    noise_values = np.random.uniform(0, 2 * np.pi, size=NUM_QUBITS)
    input_circuits_batch.append(qgan.encoding_layer(noise_values))

# convert to tensorflow quantum tensor
generator_inputs = tfq.convert_to_tensor(input_circuits_batch)
# generate fake samples using the generator
batch_generated = qgan.generator(generator_inputs)
# concatenate all time series data into one
generated_data = tf.reshape(batch_generated, shape=(num_samples*WINDOW_LENGTH,))
generated_data = tf.cast(generated_data, dtype=tf.float64)
# rescale
generated_data = rescale(generated_data, transformed_qiio)
# reverse the preprocessing on generated sample
original_norm = lambert_w_transform(generated_data, 1)
fake_original = denormalize(original_norm, tf.reduce_mean(qiio_log_r_tf), tf.math.reduce_std(qiio_log_r_tf))

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))
###################################################################################################################
#
# plot original log-returns on the left and the generated on the right
#
###################################################################################################################
axes[0].plot(date[1:], qiio_log_r_tf)
axes[0].set_xlabel('Days')
axes[0].set_title('Original Log-Returns')
axes[0].grid()

axes[1].plot(fake_original)
axes[1].set_xlabel('Days')
axes[1].set_title('Generated Log-Returns')
axes[1].grid()

plt.savefig("plots/Original Log-Returns_and_Generated Log-Returns.svg", format="svg")
plt.close()

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))
###################################################################################################################
#
# plot histogram of generated along with original log-returns on the left and the Q-Q plot on the right
#
###################################################################################################################
bin_edges = np.linspace(-0.05, 0.05, num=50)  # define the bin edges
bin_width = bin_edges[1] - bin_edges[0]
bin_edges = np.append(bin_edges, bin_edges[-1] + bin_width)
axes[0].hist(fake_original, bins=bin_edges, density=True, width=0.001, label='Generated', alpha=0.9)
axes[0].hist(qiio_log_r_tf, bins=bin_edges, density=True, width=0.001, label='Original', alpha=0.8)
axes[0].set_title('Original vs Generated Density')
axes[0].grid()
axes[0].legend()

probplot(fake_original, dist='norm', plot=axes[1])
axes[1].set_xlabel('Theoretical Quantiles')
axes[1].grid()

plt.subplots_adjust(wspace=0.3)
plt.savefig("plots/Original vs Generated Density.svg", format="svg")
plt.close()

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))
###################################################################################################################
#
# plot autocorrelations of original log-returns on the left and the generated on the right
#
###################################################################################################################
tsaplots.plot_acf(qiio_log_r_tf, ax=axes[0], lags=18, zero=False)
axes[0].set_xlabel('Lags')
axes[0].set_title('ACF Log-Returns')
axes[0].grid()

tsaplots.plot_acf(fake_original, ax=axes[1], lags=18, zero=False)
axes[1].set_xlabel('Lags')
axes[1].set_title('ACF Log-Returns (Generated)')
axes[1].grid()

plt.savefig("plots/ACF Log-Returns (Generated).svg", format="svg")
plt.close()

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))
###################################################################################################################
#
# plot volatility clustering of original log-returns on the left and the generated on the right
#
###################################################################################################################
tsaplots.plot_acf(tf.abs(qiio_log_r_tf), ax=axes[0], lags=18, zero=False)
axes[0].set_xlabel('Lags')
axes[0].set_title('ACF Absolute Log-Returns')
axes[0].grid()

tsaplots.plot_acf(tf.abs(fake_original), ax=axes[1], lags=18, zero=False)
axes[1].set_xlabel('Lags')
axes[1].set_title('ACF Absolute Log-Returns (Generated)')
axes[1].grid()

plt.savefig("plots/ACF Absolute Log-Returns (Generated).svg", format="svg")
plt.close()

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))
###################################################################################################################
#
# plot leverage effect of original log-returns on the left and the generated on the right
#
###################################################################################################################
# compute leverage effect for maximum lags = 18
leverage_original = []
for lag in range(1, 19):
    # slice the tensors to get the appropriate lagged sequences
    r_t = qiio_log_r_tf[:-lag]
    squared_lag_r = tf.square(tf.abs(qiio_log_r_tf[lag:]))

    # calculate the leverage effect
    # calculate the correlation coefficient
    correlation_matrix = np.corrcoef(r_t, squared_lag_r)
    leverage_original.append(correlation_matrix[0, 1])

leverage_generated = []
for lag in range(1, 19):
    # slice the tensors to get the appropriate lagged sequences
    r_t = fake_original[:-lag]
    squared_lag_r = tf.square(tf.abs(fake_original[lag:]))

    # calculate the leverage effect
    # calculate the correlation coefficient
    correlation_matrix = np.corrcoef(r_t, squared_lag_r)
    leverage_generated.append(correlation_matrix[0, 1])

axes[0].plot(leverage_original)
axes[0].set_xlabel('Lags')
axes[0].set_title('Original Leverage Effect')
axes[0].grid()

axes[1].plot(leverage_generated)
axes[1].set_xlabel('Lags')
axes[1].set_title('Generated Leverage Effect')
axes[1].grid()

plt.savefig("plots/Generated Leverage Effect.svg", format="svg")
plt.close()

